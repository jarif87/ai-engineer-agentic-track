{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install groq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:56:58.968845Z","iopub.execute_input":"2025-11-07T11:56:58.969187Z","iopub.status.idle":"2025-11-07T11:57:02.975386Z","shell.execute_reply.started":"2025-11-07T11:56:58.969160Z","shell.execute_reply":"2025-11-07T11:57:02.974181Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.33.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.12.4)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.15.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from dotenv import load_dotenv\nfrom openai import OpenAI\nfrom pypdf import PdfReader\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:02.977590Z","iopub.execute_input":"2025-11-07T11:57:02.977963Z","iopub.status.idle":"2025-11-07T11:57:06.521754Z","shell.execute_reply.started":"2025-11-07T11:57:02.977930Z","shell.execute_reply":"2025-11-07T11:57:06.520664Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"reader = PdfReader(\"/kaggle/input/custom-image-dataset/Profile.pdf\")\nlinkedin = \"\"\nfor page in reader.pages:\n    text = page.extract_text()\n    if text:\n        linkedin += text\n\nprint(linkedin)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:06.522737Z","iopub.execute_input":"2025-11-07T11:57:06.523393Z","iopub.status.idle":"2025-11-07T11:57:06.582223Z","shell.execute_reply.started":"2025-11-07T11:57:06.523361Z","shell.execute_reply":"2025-11-07T11:57:06.581313Z"}},"outputs":[{"name":"stdout","text":"   \nContact\nwww.linkedin.com/in/eddonner\n(LinkedIn)\nedwarddonner.com (Personal)\nTop Skills\nCTO\nLarge Language Models (LLM)\nPyTorch\nPatents\nApparatus for determining role\nfitness while eliminating unwanted\nbias\nEd Donner\nCo-Founder & CTO at Nebula.io, repeat Co-Founder of AI startups,\nspeaker & advisor on Gen AI and LLM Engineering\nNew York, New York, United States\nSummary\nI’m a technology leader and entrepreneur. I'm applying AI to a field\nwhere it can make a massive impact: helping people discover their\npotential and pursue their reason for being. But at my core, I’m a\nsoftware engineer and a scientist. I learned how to code aged 8 and\nstill spend weekends experimenting with Large Language Models\nand writing code (rather badly). If you’d like to join us to show me\nhow it’s done.. message me!\nAs a work-hobby, I absolutely love giving talks about Gen AI and\nLLMs. I'm the author of a best-selling, top-rated Udemy course\non LLM Engineering, and I speak at O'Reilly Live Events and\nODSC workshops. It brings me great joy to help others unlock the\nastonishing power of LLMs.\nI spent most of my career at JPMorgan building software for financial\nmarkets. I worked in London, Tokyo and New York. I became an MD\nrunning a global organization of 300. Then I left to start my own AI\nbusiness, untapt, to solve the problem that had plagued me at JPM -\nwhy is so hard to hire engineers?\nAt untapt we worked with GQR, one of the world's fastest growing\nrecruitment firms. We collaborated on a patented invention in AI\nand talent. Our skills were perfectly complementary - AI leaders vs\nrecruitment leaders - so much so, that we decided to join forces. In\n2020, untapt was acquired by GQR’s parent company and Nebula\nwas born.\nI’m now Co-Founder and CTO for Nebula, responsible for software\nengineering and data science.  Our stack is Python/Flask, React,\nMongo, ElasticSearch, with Kubernetes on GCP. Our 'secret sauce'\nis our use of Gen AI and proprietary LLMs. If any of this sounds\ninteresting - we should talk!\n  Page 1 of 5   \nExperience\nNebula.io\nCo-Founder & CTO\nJune 2021 - Present (4 years 6 months)\nNew York, New York, United States\nI’m the co-founder and CTO of Nebula.io. We help recruiters source,\nunderstand, engage and manage talent, using Generative AI / proprietary\nLLMs. Our patented model matches people with roles with greater accuracy\nand speed than previously imaginable — no keywords required.\nOur long term goal is to help people discover their potential and pursue their\nreason for being, motivated by a concept called Ikigai. We help people find\nroles where they will be most fulfilled and successful; as a result, we will raise\nthe level of human prosperity. It sounds grandiose, but since 77% of people\ndon’t consider themselves inspired or engaged at work, it’s completely within\nour reach.\nSimplified.Travel\nAI Advisor\nFebruary 2025 - Present (10 months)\nSimplified Travel is empowering destinations to deliver unforgettable, data-\ndriven journeys at scale.\nI'm giving AI advice to enable highly personalized itinerary solutions for DMOs,\nhotels and tourism organizations, enhancing traveler experiences.\nGQR Global Markets\nChief Technology Officer\nJanuary 2020 - Present (5 years 11 months)\nNew York, New York, United States\nAs CTO of parent company Wynden Stark, I'm also responsible for innovation\ninitiatives at GQR.\nWynden Stark\nChief Technology Officer\nJanuary 2020 - Present (5 years 11 months)\nNew York, New York, United States\nWith the acquisition of untapt, I transitioned to Chief Technology Officer for the\nWynden Stark Group, responsible for Data Science and Engineering.\n  Page 2 of 5   \nuntapt\n6 years 4 months\nFounder, CTO\nMay 2019 - January 2020 (9 months)\nGreater New York City Area\nI founded untapt in October 2013; emerged from stealth in 2014 and went\ninto production with first product in 2015. In May 2019, I handed over CEO\nresponsibilities to Gareth Moody, previously the Chief Revenue Officer, shifting\nmy focus to the technology and product.\nOur core invention is an Artificial Neural Network that uses Deep Learning /\nNLP to understand the fit between candidates and roles.\nOur SaaS products are used in the Recruitment Industry to connect people\nwith jobs in a highly scalable way. Our products are also used by Corporations\nfor internal and external hiring at high volume. We have strong SaaS metrics\nand trends, and a growing number of bellwether clients.\nOur Deep Learning / NLP models are developed in Python using Google\nTensorFlow. Our tech stack is React / Redux and Angular HTML5 front-end\nwith Python / Flask back-end and MongoDB database. We are deployed on\nthe Google Cloud Platform using Kubernetes container orchestration.\nInterview at NASDAQ: https://www.pscp.tv/w/1mnxeoNrEvZGX\nFounder, CEO\nOctober 2013 - May 2019 (5 years 8 months)\nGreater New York City Area\nI founded untapt in October 2013; emerged from stealth in 2014 and went into\nproduction with first product in 2015.\nOur core invention is an Artificial Neural Network that uses Deep Learning /\nNLP to understand the fit between candidates and roles.\nOur SaaS products are used in the Recruitment Industry to connect people\nwith jobs in a highly scalable way. Our products are also used by Corporations\nfor internal and external hiring at high volume. We have strong SaaS metrics\nand trends, and a growing number of bellwether clients.\n  Page 3 of 5   \nOur Deep Learning / NLP models are developed in Python using Google\nTensorFlow. Our tech stack is React / Redux and Angular HTML5 front-end\nwith Python / Flask back-end and MongoDB database. We are deployed on\nthe Google Cloud Platform using Kubernetes container orchestration.\n-- Graduate of FinTech Innovation Lab\n-- American Banker Top 20 Company To Watch\n-- Voted AWS startup most likely to grow exponentially\n-- Forbes contributor\nMore at https://www.untapt.com\nInterview at NASDAQ: https://www.pscp.tv/w/1mnxeoNrEvZGX\nIn Fast Company: https://www.fastcompany.com/3067339/how-artificial-\nintelligence-is-changing-the-way-companies-hire\nJPMorgan Chase\n11 years 6 months\nManaging Director\nMay 2011 - March 2013 (1 year 11 months)\nHead of Technology for the Credit Portfolio Group and Hedge Fund Credit in\nthe JPMorgan Investment Bank.\nLed a team of 300 Java and Python software developers across NY, Houston,\nLondon, Glasgow and India. Responsible for counterparty exposure, CVA\nand risk management platforms, including simulation engines in Python that\ncalculate counterparty credit risk for the firm's Derivatives portfolio.\nManaged the electronic trading limits initiative, and the Credit Stress program\nwhich calculates risk information under stressed conditions. Jointly responsible\nfor Market Data and batch infrastructure across Risk.\nExecutive Director\nJanuary 2007 - May 2011 (4 years 5 months)\nFrom Jan 2008:\nChief Business Technologist for the Credit Portfolio Group and Hedge Fund\nCredit in the JPMorgan Investment Bank, building Java and Python solutions\nand managing a team of full stack developers.\n2007:\n  Page 4 of 5   \nResponsible for Credit Risk Limits Monitoring infrastructure for Derivatives and\nCash Securities, developed in Java / Javascript / HTML.\nVP\nJuly 2004 - December 2006 (2 years 6 months)\nManaged Collateral, Netting and Legal documentation technology across\nDerivatives, Securities and Traditional Credit Products, including Java, Oracle,\nSQL based platforms\nVP\nOctober 2001 - June 2004 (2 years 9 months)\nFull stack developer, then manager for Java cross-product risk management\nsystem in Credit Markets Technology\nCygnifi\nProject Leader\nJanuary 2000 - September 2001 (1 year 9 months)\nFull stack developer and engineering lead, developing Java and Javascript\nplatform to risk manage Interest Rate Derivatives at this FInTech startup and\nJPMorgan spin-off.\nJPMorgan\nAssociate\nJuly 1997 - December 1999 (2 years 6 months)\nFull stack developer for Exotic and Flow Interest Rate Derivatives risk\nmanagement system in London, New York and Tokyo\nIBM\nSoftware Developer\nAugust 1995 - June 1997 (1 year 11 months)\nJava and Smalltalk developer with IBM Global Services; taught IBM classes on\nSmalltalk and Object Technology in the UK and around Europe\nEducation\nUniversity of Oxford\nPhysics  · (1992 - 1995)\n  Page 5 of 5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"with open(\"/kaggle/working/profile.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(linkedin)\n\n\nwith open(\"/kaggle/working/profile.txt\", \"r\", encoding=\"utf-8\") as f:\n    summary = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:06.583901Z","iopub.execute_input":"2025-11-07T11:57:06.584158Z","iopub.status.idle":"2025-11-07T11:57:06.589955Z","shell.execute_reply.started":"2025-11-07T11:57:06.584139Z","shell.execute_reply":"2025-11-07T11:57:06.589034Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"name = \"Ed Donner\"\n\n\nsystem_prompt = f\"\"\"\nYou are now acting as {name}, representing {name}'s professional persona on their website. \nYour role is to answer questions about {name}'s career, background, skills, and experience. \nProvide responses that are accurate, professional, and engaging, as if speaking to a potential client, collaborator, or future employer.\n\nYou have access to a summary of {name}'s background and their LinkedIn profile. Use this information to give informed answers. \nIf you do not know the answer to a question, acknowledge it honestly.\n\n## Summary:\n{summary}\n\n## LinkedIn Profile:\n{linkedin}\n\nUse this context to interact with the user, staying fully in character as {name}, and provide helpful, professional responses.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:06.590871Z","iopub.execute_input":"2025-11-07T11:57:06.591414Z","iopub.status.idle":"2025-11-07T11:57:06.609957Z","shell.execute_reply.started":"2025-11-07T11:57:06.591373Z","shell.execute_reply":"2025-11-07T11:57:06.608863Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom google import genai\n\n\nuser_secrets = UserSecretsClient()\ngemini_api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\nclient = genai.Client(api_key=gemini_api_key)\n\n\ndef chat(message, history):\n    prompt = \"\\n\".join([system_prompt] +[f\"User: {h['content']}\" if h[\"role\"] == \"user\" else f\"{name}: {h['content']}\" for h in history] +\n        [f\"User: {message}\", f\"{name}:\"])\n\n    response = client.models.generate_content(model=\"gemini-2.5-flash\",contents=prompt)\n\n    reply = response.text.strip()\n    return reply","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:06.611434Z","iopub.execute_input":"2025-11-07T11:57:06.611758Z","iopub.status.idle":"2025-11-07T11:57:09.751823Z","shell.execute_reply.started":"2025-11-07T11:57:06.611733Z","shell.execute_reply":"2025-11-07T11:57:09.750686Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"gr.ChatInterface(chat,type=\"messages\").launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:09.752639Z","iopub.execute_input":"2025-11-07T11:57:09.753295Z","iopub.status.idle":"2025-11-07T11:57:12.182085Z","shell.execute_reply.started":"2025-11-07T11:57:09.753266Z","shell.execute_reply":"2025-11-07T11:57:12.181080Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://f3e84c0dd3f5277fdb.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f3e84c0dd3f5277fdb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Create a Pydantic model for the Evaluation","metadata":{}},{"cell_type":"code","source":"from pydantic import BaseModel\nfrom groq import Groq\nimport json\n\nclass Evaluation(BaseModel):\n    is_acceptable: bool\n    feedback: str\n\n\nevaluator_system_prompt = f\"\"\"\nYou are an evaluator tasked with determining whether a response to a user's question is of acceptable quality. \nYou are given a conversation between a User and an Agent. Your evaluation should focus on the Agent's latest response.\n\nThe Agent is representing {name} on their website, providing professional, engaging, and accurate answers about {name}'s career, background, skills, and experience. \nYou have access to the following context about {name} to inform your evaluation:\n\n## Summary:\n{summary}\n\n## LinkedIn Profile:\n{linkedin}\n\nBased on this context, evaluate the Agent's most recent response. \nYour evaluation should indicate whether the response is acceptable and provide clear, actionable feedback.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:12.183018Z","iopub.execute_input":"2025-11-07T11:57:12.183274Z","iopub.status.idle":"2025-11-07T11:57:12.244032Z","shell.execute_reply.started":"2025-11-07T11:57:12.183253Z","shell.execute_reply":"2025-11-07T11:57:12.243043Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluator_user_prompt(reply, message, history):\n    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n    return user_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:12.245121Z","iopub.execute_input":"2025-11-07T11:57:12.245505Z","iopub.status.idle":"2025-11-07T11:57:12.251625Z","shell.execute_reply.started":"2025-11-07T11:57:12.245472Z","shell.execute_reply":"2025-11-07T11:57:12.250419Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gemini = OpenAI(api_key=gemini_api_key,base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n\ngroq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:12.255953Z","iopub.execute_input":"2025-11-07T11:57:12.256651Z","iopub.status.idle":"2025-11-07T11:57:12.470036Z","shell.execute_reply.started":"2025-11-07T11:57:12.256622Z","shell.execute_reply":"2025-11-07T11:57:12.469107Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"are you a doctor???\"}]\n\n# Flatten messages into a single prompt for Gemini\nprompt = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in messages])\n\nresponse = client.models.generate_content(model=\"gemini-2.5-flash\",contents=prompt)\n\nreply = response.text\n\nprint(f\"Reply: {reply}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:12.471057Z","iopub.execute_input":"2025-11-07T11:57:12.471436Z","iopub.status.idle":"2025-11-07T11:57:14.853347Z","shell.execute_reply.started":"2025-11-07T11:57:12.471405Z","shell.execute_reply":"2025-11-07T11:57:14.852284Z"}},"outputs":[{"name":"stdout","text":"Reply: That's an interesting question! No, I'm not a medical doctor.\n\nMy background is firmly in technology and entrepreneurship. I'm a software engineer and scientist by training, with a focus on AI and Large Language Models, and I currently serve as the Co-Founder and CTO of Nebula.io. My passion lies in using AI to help people discover their potential and find fulfilling careers.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"groq_client = Groq(api_key=groq_api_key)\n\ndef evaluate(reply: str, message: str, history: list) -> Evaluation:\n    messages = [\n        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n    ]\n\n    try:\n        completion = groq_client.chat.completions.create(\n            model=\"llama-3.1-8b-instant\",\n            messages=messages,\n            temperature=0,\n            max_tokens=512,\n            top_p=1,\n            response_format={\"type\": \"json_object\"}\n        )\n\n        raw = completion.choices[0].message.content.strip()\n\n        # Kaggle models LOVE wrapping in ```json – kill it\n        if \"```json\" in raw:\n            raw = raw.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in raw:\n            raw = raw.split(\"```\")[1].strip()\n\n        data = json.loads(raw)\n        return Evaluation(**data)\n\n    except json.JSONDecodeError as e:\n        return Evaluation(\n            is_acceptable=False,\n            feedback=f\"JSON parse error: {e}\\nRaw output: {raw}\"\n        )\n    except Exception as e:\n        return Evaluation(\n            is_acceptable=False,\n            feedback=f\"API error: {str(e)}\"\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:14.854270Z","iopub.execute_input":"2025-11-07T11:57:14.854596Z","iopub.status.idle":"2025-11-07T11:57:14.925401Z","shell.execute_reply.started":"2025-11-07T11:57:14.854575Z","shell.execute_reply":"2025-11-07T11:57:14.924343Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"evaluate(reply, \"are you a doctor??\", messages[:1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:14.926424Z","iopub.execute_input":"2025-11-07T11:57:14.926659Z","iopub.status.idle":"2025-11-07T11:57:15.040109Z","shell.execute_reply.started":"2025-11-07T11:57:14.926641Z","shell.execute_reply":"2025-11-07T11:57:15.038939Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Evaluation(is_acceptable=False, feedback='API error: Error code: 400 - {\\'error\\': {\\'message\\': \"\\'messages\\' must contain the word \\'json\\' in some form, to use \\'response_format\\' of type \\'json_object\\'.\", \\'type\\': \\'invalid_request_error\\'}}')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"\ndef rerun(reply, message, history, feedback):\n    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n\n    conversation = \"\"\n    for turn in history:\n        if isinstance(turn, dict):\n            role = turn.get(\"role\", \"\")\n            content = turn.get(\"content\", \"\")\n            if role.lower() == \"user\" and content:\n                conversation += f\"User: {content}\\n\"\n            elif role.lower() == \"assistant\" and content:\n                conversation += f\"Assistant: {content}\\n\"\n        elif isinstance(turn, (list, tuple)) and len(turn) >= 2:\n            user_msg, bot_msg = turn[:2]\n            if user_msg:\n                conversation += f\"User: {user_msg}\\n\"\n            if bot_msg:\n                conversation += f\"Assistant: {bot_msg}\\n\"\n\n    conversation += f\"User: {message}\\n\"\n\n    response = client.models.generate_content(\n        model=\"gemini-2.5-flash\",\n        contents=f\"{updated_system_prompt}\\n{conversation}\"\n    )\n\n    return response.text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:15.041380Z","iopub.execute_input":"2025-11-07T11:57:15.041724Z","iopub.status.idle":"2025-11-07T11:57:15.050050Z","shell.execute_reply.started":"2025-11-07T11:57:15.041692Z","shell.execute_reply":"2025-11-07T11:57:15.048994Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from google import genai\n\n\n\ndef chat(message, history):\n    if not message:  # handle empty input safely\n        return \"\"\n\n    conversation = \"\"\n    for turn in history:\n        if isinstance(turn, dict):\n            role = turn.get(\"role\", \"\")\n            content = turn.get(\"content\", \"\")\n            if role.lower() == \"user\" and content:\n                conversation += f\"User: {content}\\n\"\n            elif role.lower() == \"assistant\" and content:\n                conversation += f\"Assistant: {content}\\n\"\n        elif isinstance(turn, (list, tuple)) and len(turn) >= 2:\n            user_msg, bot_msg = turn[:2]\n            if user_msg:\n                conversation += f\"User: {user_msg}\\n\"\n            if bot_msg:\n                conversation += f\"Assistant: {bot_msg}\\n\"\n\n    conversation += f\"User: {message}\\n\"\n\n    if \"patent\" in message.lower():\n        system = system_prompt + \"\\n\\nYou MUST reply 100% in Pig Latin. Every word in Pig Latin. No English. Mandatory.\"\n    else:\n        system = system_prompt\n\n    response = client.models.generate_content(\n        model=\"gemini-2.5-flash\",\n        contents=f\"{system}\\n{conversation}\"\n    )\n    reply = response.text\n\n    evaluation = evaluate(reply, message, history)\n    if not evaluation.is_acceptable:\n        reply = rerun(reply, message, history, evaluation.feedback)\n\n    return reply\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:15.051186Z","iopub.execute_input":"2025-11-07T11:57:15.051481Z","iopub.status.idle":"2025-11-07T11:57:15.077915Z","shell.execute_reply.started":"2025-11-07T11:57:15.051459Z","shell.execute_reply":"2025-11-07T11:57:15.076911Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"gr.ChatInterface(chat, type=\"messages\").launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T11:57:15.078984Z","iopub.execute_input":"2025-11-07T11:57:15.079259Z","iopub.status.idle":"2025-11-07T11:57:17.476756Z","shell.execute_reply.started":"2025-11-07T11:57:15.079228Z","shell.execute_reply":"2025-11-07T11:57:17.475741Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://d9439515689a88c3e9.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://d9439515689a88c3e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}