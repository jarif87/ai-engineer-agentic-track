{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeae4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.tools import tool\n",
    "from IPython.display import display, Markdown\n",
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime\n",
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ALL_IN_ONE_WORKER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a33aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899d789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\n",
    "host.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56350197",
   "metadata": {},
   "outputs": [],
   "source": [
    "serper = GoogleSerperAPIWrapper()\n",
    " \n",
    "@tool\n",
    "def langchain_serper(query: str) -> str:\n",
    "    \"\"\"Fetch relevant information from the web for a given query\"\"\"\n",
    "    return serper.run(query)\n",
    "\n",
    "\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c4fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction1 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons in favor of choosing AutoGen; the pros of AutoGen.\"\n",
    "\n",
    "instruction2 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons against choosing AutoGen; the cons of Autogen.\"\n",
    "\n",
    "judge = \"You must make a decision on whether to use AutoGen for a project. \\\n",
    "Your research team has come up with the following reasons for and against. \\\n",
    "Based purely on the research from your team, please respond with your decision and brief rationale.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537b8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Judge(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        message1 = Message(content=instruction1)\n",
    "        message2 = Message(content=instruction2)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message1, inner_1)\n",
    "        response2 = await self.send_message(message2, inner_2)\n",
    "        result = f\"## Pros of AutoGen:\\n{response1.content}\\n\\n## Cons of AutoGen:\\n{response2.content}\\n\\n\"\n",
    "        judgement = f\"{judge}\\n{result}Respond with your decision and brief explanation\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + \"\\n\\n## Decision:\\n\\n\" + response.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f13adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if ALL_IN_ONE_WORKER:\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "\n",
    "    await Player1Agent.register(worker, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "    await Player2Agent.register(worker, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "else:\n",
    "\n",
    "    worker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker1.start()\n",
    "    await Player1Agent.register(worker1, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "\n",
    "    worker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker2.start()\n",
    "    await Player2Agent.register(worker2, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8742089",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await worker.send_message(Message(content=\"Go!\"), agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e2bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pros of AutoGen:\n",
       "Here are several pros of using AutoGen in your AI Agent project:\n",
       "\n",
       "1. **Customizability**: AutoGen allows for the creation of highly customizable AI agents that can be tailored to specific project needs.\n",
       "\n",
       "2. **Multi-Modal Operation**: AutoGen supports different operational modes, enabling agents to utilize combinations of Large Language Models (LLMs), human inputs, and various tools efficiently.\n",
       "\n",
       "3. **Microsoft Backing**: Being backed by Microsoft, AutoGen benefits from extensive resources and support, making it a reliable choice with a strong ecosystem.\n",
       "\n",
       "4. **Open-Source Framework**: As an open-source platform, AutoGen fosters collaboration and improvement through contributions from the development community.\n",
       "\n",
       "5. **Tool Integration**: It supports an array of tools including code executors and function callers, which allows agents to perform complex tasks autonomously, enhancing automation capabilities.\n",
       "\n",
       "6. **Efficiency & Scalability**: AutoGen enhances the efficiency of developing AI applications and scales well to accommodate growing project requirements.\n",
       "\n",
       "7. **Seamless Azure Integration**: The framework integrates smoothly with Microsoft Azure AI services, allowing easy deployment and leveraging cloud capabilities.\n",
       "\n",
       "8. **Enhanced Collaboration**: AutoGen excels in enabling multi-agent collaboration, which can shorten feedback loops and facilitate more effective teamwork among agents.\n",
       "\n",
       "These attributes make AutoGen a compelling choice for developing AI agents in your project. \n",
       "\n",
       "TERMINATE\n",
       "\n",
       "## Cons of AutoGen:\n",
       "Here are some cons of using AutoGen for your AI Agent project:\n",
       "\n",
       "1. **Steep Learning Curve**: AutoGen can be complex, especially for developers not familiar with state machines or graph-based logic. This may slow down development and require additional training.\n",
       "\n",
       "2. **Documentation Issues**: The documentation for AutoGen is often criticized as being difficult to read and lacking sufficient examples, which can hinder understanding and implementation.\n",
       "\n",
       "3. **Limited Built-in Compliance**: AutoGen has restrictions regarding compliance features, which may be a concern for projects that require strict adherence to regulatory standards.\n",
       "\n",
       "4. **Security Limitations**: There are potential security issues, including the absence of explicit features like data encryption or intellectual property control.\n",
       "\n",
       "5. **High Costs**: The cost associated with using AutoGen can be significant, particularly regarding API usage and token limits, making it less accessible for budget-constrained projects.\n",
       "\n",
       "6. **Incompatibility with Open Source Models**: AutoGen may not seamlessly integrate with open-source AI models, limiting flexibility and customization options.\n",
       "\n",
       "7. **Best Suited for Research Environments**: It may be more appropriate for experimental applications rather than straightforward commercial deployments, which could limit its practicality for some projects.\n",
       "\n",
       "These factors should be considered carefully when deciding whether to adopt AutoGen in your AI Agent project. \n",
       "\n",
       "TERMINATE\n",
       "\n",
       "\n",
       "\n",
       "## Decision:\n",
       "\n",
       "Based on the research provided by the team, the decision is to **use AutoGen** for the project.\n",
       "\n",
       "### Rationale:\n",
       "The benefits of AutoGen significantly outweigh the drawbacks for our AI Agent project. Its customizability and multi-modal operation are essential for tailoring agents to specific needs, while the backing by Microsoft ensures reliability and support. The open-source nature promotes collaboration, and the tool integration capabilities enhance automation, making it a strong choice for developing sophisticated AI applications.\n",
       "\n",
       "Although there are concerns regarding the learning curve, documentation, and compliance, these can be managed through dedicated training and support resources. Furthermore, the advantages of efficiency, scalability, and seamless Azure integration position AutoGen well for the project's requirements. Therefore, despite some cons, the potential to innovate and create effective AI agents makes using AutoGen a compelling option.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import signal\n",
    "import os\n",
    "\n",
    "def nuclear_shutdown():\n",
    "    print(\"NUCLEAR SHUTDOWN INITIATED – killing everything...\")\n",
    "    \n",
    "    # Method 1: Force-kill the gRPC servers (most important)\n",
    "    workers = [worker] if ALL_IN_ONE_WORKER else [worker1, worker2, worker]\n",
    "    for w in workers:\n",
    "        if hasattr(w, \"_server\") and w._server:\n",
    "            try:\n",
    "                w._server.stop(grace=0)\n",
    "                print(f\"   → gRPC server forced stop\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Method 2: Kill the current event loop tasks (kills hanging coroutines)\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        for task in asyncio.all_tasks(loop):\n",
    "            task.cancel()\n",
    "        print(\"   → All async tasks cancelled\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Method 3: OS-level kill (Windows safe)\n",
    "    print(\"   → Sending SIGTERM to self...\")\n",
    "    os.kill(os.getpid(), signal.SIGTERM)\n",
    "\n",
    "# RUN THIS WHEN IT HANGS\n",
    "nuclear_shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef0f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await host.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef291e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
